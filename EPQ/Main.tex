\documentclass[12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{graphicx}   % For images
\usepackage{lmodern}    % Modern font
\usepackage{setspace}   % Line spacing
\usepackage{titling}    % Custom title page support

\begin{document}

%-------------------------
% TITLE PAGE
%-------------------------
\begin{titlepage}
    \centering
    \vspace*{3cm}
    
    % TITLE
    {\Huge\bfseries What are the main theoretical barriers to resolving P vs NP?\par}
    \vspace{1.5cm}
    
    % SUBTITLE (Optional)
    {\Large\itshape An Extended Project Qualification Dissertation\par}
    \vspace{2cm}
    
    % AUTHOR INFO
    {\Large Camron Short\par}
    \vspace{0.5cm}
    {\large Candidate Number: 6439\par}
    {\large Centre Number: 19216\par}
    \vspace{2cm}
    
    % NO PAGE NUMBER
    \thispagestyle{empty}
\end{titlepage}

\section*{Defining the problem}
\begin{center}
    %\vspace*{3cm}
    %{\Huge\bfseries Defining the problem\par}
    \vspace{0.5cm}
    {\Large\itshape Big O-Notation\par}
\end{center}
When shopping, there are multiple ways to collect all the items on your list.
You might start at one end of the store and work through it methodically, which is often the most efficient - but only if your list is already sorted by store layout.
Without planning, you might find yourself doubling back or criss-crossing the store multiple times.
These differences in method lead to drastically different total effort.
Similarly, the time taken depends not just on the method, but also on who is doing the shopping and how many items are on the list.
These human and environmental factors make it impossible to measure an algorithm's speed purely in seconds.
To address this, computer scientists use \textbf{Big O notation}, a mathematical way to describe how the number of steps an algorithm takes grows with the size of the input.
For example, an algorithm with complexity $O(n)$ will perform at most a number of operations proportional to the length of the input list ($n$).
An algorithm with $O(n^2)$ complexity might check each item against every other, leading to significantly more steps.
These expressions describe the algorithm’s \textbf{asymptotic behaviour} - that is, how it scales as inputs grow very large.
When this growth follows a polynomial pattern like $O(n)$ or $O(n^2)$, we say it runs in \textbf{polynomial time}, which is generally considered efficient and tractable.

\begin{center}
    \vspace{0cm}
    {\Large\itshape Sets and Set Notation\par}
\end{center}
Earlier, we used a shopping list to explore algorithmic strategies. Mathematically, that list can be viewed as a \textbf{set} - a well-defined collection of distinct objects, such as items to buy.
In computer science and mathematics, Sets are fundamental.
A set can contain numbers, items, states, algorithms, or even other sets.
For example, the set of paths through a store might include every possible route you could take.
The set of solutions to a problem includes all algorithms that solve it.
We describe relationships between sets and their elements using \textbf{Set notation}:
\begin{itemize}
    \item $\in$: “is an element of” (e.g., $3 \in \{1, 2, 3\}$ means 3 is in the set)
    \item $\notin$: “is not an element of” (e.g., $4 \notin \{1, 2, 3\}$)
    \item $\subseteq$: “is a subset of” (e.g., $\{1,2\} \subseteq \{1,2,3\}$)
    \item $\cup$: union (e.g., $A \cup B$ contains all elements in $A$ or $B$)
    \item $\cap$: intersection (e.g., $A \cap B$ contains only elements in both $A$ and $B$)
    \item $\setminus$: set difference (e.g., $A \setminus B$ contains elements in $A$ but not in $B$)
\end{itemize}

\begin{center}
    \vspace{0cm}
    {\Large\itshape P\par}
\end{center}
The first set in our investigation is called \textbf{P}. This set contains all problems that can be solved by an algorithm in \textbf{polynomial time} - meaning the number of steps required grows at most like $n$, $n^2$, $n^3$, etc., where $n$ is the size of the input.
A good example is the weekly shop.
Planning the optimal route through the store may take time, but it is feasible to compute in polynomial time - for example, with an algorithm of complexity $O(n^2)$.

\begin{center}
    \vspace{0cm}
    {\Large\itshape NP\par}
\end{center}
The (potentially larger) set \textbf{NP} stands for \textbf{nondeterministic polynomial time}. This set contains all problems where a proposed solution can be \textit{verified} in polynomial time, even if finding that solution might be difficult.
Consider Sudoku: finding a solution from scratch may be hard, but checking whether a filled grid obeys the rules is quick. Similarly, navigating a store inefficiently still allows you to verify whether you've bought all items - even if the path wasn't optimal.
We know that $P \subseteq NP$: any problem we can solve efficiently, we can also verify efficiently. Whether or not these sets are actually equal is the essence of our central question.

\begin{center}
    \vspace{0cm}
    {\Large\itshape Our Question\par}
\end{center}

This brings us to our guiding problem: \textbf{What is the relationship between $P$ and $NP$?}
Mathematically, we know that $P$ is a subset of $NP$, but we do not know whether $P = NP$. That is, we don't know whether every efficiently verifiable problem can also be efficiently solved.
Some mathematicians believe $\mathbf{P = NP}$ - implying that every hard-looking problem has a hidden efficient solution - while others believe $\mathbf{P \neq NP}$, suggesting some problems are inherently hard to solve even if easy to check.
This project does not aim to resolve the question. Instead, we will explore \textit{why} it has remained unanswered for decades - and examine the theoretical barriers that prevent us from settling it.

\section*{Relativization}

\end{document}

\bibliographystyle{unsrtnat}
\bibliography{references}
\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}
